# Model
model_name: ByteDance/Ouro-2.6B
total_ut_steps: 4

# Dataset
dataset: airesearch/WangchanThaiInstruct

# Quantization
use_qlora: true

# LoRA
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05

# Training
batch_size: 2
gradient_accumulation: 8
max_seq_length: 2048
learning_rate: 2e-4
lr_scheduler: cosine
warmup_ratio: 0.05
num_train_epochs: 3
max_steps: -1

# Logging & saving
logging_steps: 10
save_steps: 200
report_to: wandb
output_dir: ./output/ouro-2.6b-thai-ut4
